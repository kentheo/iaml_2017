{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Introductory applied machine learning (INFR10069)](https://www.learn.ed.ac.uk/webapps/blackboard/execute/content/blankPage?cmd=view&content_id=_2651677_1&course_id=_53633_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by [James Owers](https://jamesowers.github.io/), University of Edinburgh 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "    * [Lab Outline](#Lab-Outline)\n",
    "    * [The Data](#The-Data)\n",
    "1. [Part 1 - Introducing the Neural Network Model](#Part-1---Introducing-the-Neural-Network-Model)\n",
    "    * [Resources to Watch and Read pt. 1](##Resources-to-Watch-and-Read-pt.-1)\n",
    "    * [Model Design](#Model-Design)\n",
    "    * [The Cost Space](#The-Cost-Space)\n",
    "1. [Part 2 - Fitting the Model & Optimisation](#Part-2---Fitting-the-Model-&-Optimisation)\n",
    "    * [Resources to Watch and Read pt. 2](#Resources-to-Watch-and-Read-pt.-2)\n",
    "    * [Finding the Best Parameters](#Finding-the-Best-Parameters)\n",
    "    * [Gradient Descent](#Gradient-Descent)\n",
    "    * [Backpropagation](#Backpropagation)\n",
    "1. [Part 3 - Implementation From Scratch](#Part-3---Implementation-From-Scratch!)\n",
    "1. [Part 4 - Implementation With Sklearn](#Part-4---Implementation-with-Sklearn)\n",
    "1. [Moar?!](#Please-sir...I-want-some-more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://docs.python.org/2/library/__future__.html\n",
    "# make printing and division act like python 3\n",
    "from __future__ import division, print_function\n",
    "\n",
    "# General\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "from IPython.display import Image, HTML\n",
    "\n",
    "# Data structures\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Modelling\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import check_grad\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Local module adjacent to this notebook\n",
    "import iaml\n",
    "from iaml.data import load_letters\n",
    "\n",
    "# http://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab:\n",
    "1. introduces a simple neural network model in a supervised learning setting\n",
    "1. provides impetus to understand the fitting procedure of that, and other networks\n",
    "1. encourages you to implement a model from scratch\n",
    "1. models the same problem with the sklearn package\n",
    "1. makes you think about what you've done!\n",
    "\n",
    "It does not discuss in detail:\n",
    "1. any of the plethora of different activation functions you can use e.g. RELUs, SELUs, Tanh, ...\n",
    "1. how to initialise the parameters and why that matters\n",
    "1. issues with the fitting process e.g. local optima, and how to avoid them e.g. learning rate schedulers, momentum, RMSProp, Adam, cyclic learning rates\n",
    "1. issues with model complexity e.g. overfitting, and solutions such as dropout, regularisation, or using [shedloads of data](https://what-if.xkcd.com/63/)\n",
    "1. other tricks for speeding up and stablising fitting such as batch sizes, weight norm, layer norm\n",
    "1. deep networks and their tricks like skip connections, pooling, convolutions\n",
    "1. nor other more complex architectures like CNNs, RNNs, LSTMs, GANs, etc. etc.\n",
    "1. many, many, MANY other things (that probably were published, like, [yesterday](https://arxiv.org/abs/1711.04340v1))\n",
    "\n",
    "However, if you understand what is in this notebook well, **you will have the ability to understand [all of these things](https://i.imgflip.com/1zn8p9.jpg)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I provide you with a function that creates data then link you to some excellent resources to learn the basics. These resources are superb, short, and free. I highly, highly recommend setting aside a couple of hours to give them a good watch/read and, at the very least, use them for reference. \n",
    "\n",
    "After you have had a crack at the problems, I'll release the solutions. The solutions, particularly to part 3, walk you through the process of coding a simple neural neural network in detail.\n",
    "\n",
    "Parts 3 & 4 are practical, parts 1 & 2 are links to external resources to read. Whilst I recommend you soak up some context first with 1 & 2, feel free to jump in at the deep end and get your hands dirty with part 3 or 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this lab we are going to be using a simple classification example: the TC classification problem (not to be confused with the real [TC](https://www.youtube.com/watch?v=NToYkBYezZA)). This is a small toy problem where we, initially, try to distinguish between 3x3 grids that look like Ts and Cs. Let's create the dataset and have a look..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have written a function `load_letters()` to generate synthetic data. For now, you will use the data generated below, but later you have opportunity to play with generating different data if you like. The function is located in the `iaml` module adjacent to this notebook - feel free to check out the code but I advise you **do not edit it**. Run (and don't edit) the next few cells to create and observe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bounds = [-1, 1]\n",
    "X, y, y_labels = load_letters(categories=['T', 'C'], \n",
    "                              num_obs=[50, 50],\n",
    "                              bounds=bounds,\n",
    "                              beta_params=[[1, 8], [8, 1]],\n",
    "                              shuffle=True, \n",
    "                              random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the data (I'm just creating a Pandas DataFrame for display, I probably wont use this object again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>Class (numeric)</th>\n",
       "      <th>Class Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.83</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>0.84</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.91</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.97</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.83</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      x0    x1    x2     x3     x4     x5     x6    x7     x8 Class (numeric)  \\\n",
       "0   0.66  0.89  0.83  -0.54   0.18  -0.58  -0.53  0.84   -0.9               0   \n",
       "1   0.77   0.9  0.92   0.72  -0.69  -0.31   0.98  0.86   0.94               1   \n",
       "2   0.19  0.96  0.91  -0.86   0.92  -0.91  -0.99  0.89  -0.55               0   \n",
       "3   0.79  0.51  0.97  -0.68   0.69  -0.93  -0.99  0.35  -0.81               0   \n",
       "4   0.46  0.73  0.94  -0.98    0.9  -0.77  -0.63  0.67  -0.79               0   \n",
       "..   ...   ...   ...    ...    ...    ...    ...   ...    ...             ...   \n",
       "95  0.68  0.64  0.95  -0.91   0.86  -0.95  -0.95  0.55  -0.96               0   \n",
       "96   0.0  0.26  0.96  -0.98   0.76  -0.95  -0.96  0.68  -0.87               0   \n",
       "97   0.7  0.98  0.86   0.83  -0.96   -1.0   0.95  0.85   0.99               1   \n",
       "98  0.78  0.49  0.96  -0.57   0.89  -0.99  -0.99  0.98  -0.51               0   \n",
       "99  0.58   1.0  0.94  -0.79  -0.13  -0.99  -0.89  0.62  -0.91               0   \n",
       "\n",
       "   Class Label  \n",
       "0            T  \n",
       "1            C  \n",
       "2            T  \n",
       "3            T  \n",
       "4            T  \n",
       "..         ...  \n",
       "95           T  \n",
       "96           T  \n",
       "97           C  \n",
       "98           T  \n",
       "99           T  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"max_rows\", 10)\n",
    "df = pd.DataFrame(\n",
    "    np.hstack(\n",
    "        [np.around(X,2), \n",
    "         y[:, np.newaxis], \n",
    "         np.array([y_labels[ii] for ii in y])[:, np.newaxis]\n",
    "        ]\n",
    "    ),\n",
    "    columns = ['x{}'.format(ii) for ii in range(9)] + ['Class (numeric)', 'Class Label']\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.reset_option(\"max_rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are arranged as vectors for your convenience, but they're really `3 x 3` images. Here's a function to plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_grid(x, shape=None, **heatmap_params):\n",
    "    \"\"\"Function for reshaping and plotting vector data.\n",
    "    If shape not given, assumed square.\n",
    "    \"\"\"\n",
    "    if shape is None:\n",
    "        width = int(np.sqrt(len(x)))\n",
    "        if width == np.sqrt(len(x)):\n",
    "            shape = (width, width)\n",
    "        else:\n",
    "            print('Data not square, supply shape argument')\n",
    "    sns.heatmap(x.reshape(shape), annot=True, **heatmap_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWUAAAEICAYAAACH7+U/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkrcpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e6771cb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWUAAAEICAYAAACH7+U/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e6771c990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWUAAAEICAYAAACH7+U/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/H/7lQUZe8avrwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e5f49a5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for ii in range(3):\n",
    "    plt.figure()\n",
    "    plot_grid(X[ii], vmin=bounds[0], vmax=bounds[1], cmap='Greys')\n",
    "    plt.title('Observation {}: Class = {} (numeric label {})'.format(ii, y_labels[y[ii]], y[ii]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's make the train and test split. Again, don't alter this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, 9), (17, 9), (50, 9), (33,), (17,), (50,)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dd.shape for dd in [X_train, X_valid, X_test, y_train, y_valid, y_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Introducing the Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources to Watch and Read pt. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading/watching time:** 30 minutes\n",
    "\n",
    "First, watch this video from 3 Blue 1 Brown: [But what *is* a Neural Network? | Deep learning, chapter 1](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "\n",
    "If you prefer reading, try 2 sections of Nielsen's Book Chapter 1:\n",
    "* [Sigmoid Neurons](http://neuralnetworksanddeeplearning.com/chap1.html#sigmoid_neurons)\n",
    "* and [The Architecture of Neural Networks](http://neuralnetworksanddeeplearning.com/chap1.html#the_architecture_of_neural_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just so as there's something in this notebook to quickly reference - here's a nice illustration of what's going on in a neural net. Within the calculation of the $z$'s you'll see the learned **parameters**: $w$'s and $b$'s - these are the weights and biases respectively. *N.B. I omit the bias $b$ parameters in the Part 3 implementation.* The functions $g$ are the activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/neural-net.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cost Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When we talk about the cost space, loss$^*$ space, or cost surface, we are talking about a function that changes with respect to the parameters. This function determines how well the network is performing - a low cost is good, a high cost is bad. A simple example for two parameters is shown below. **Our goal is to update the parameters such that we find the global minimum of the cost function.**\n",
    "\n",
    "$^*$ 'loss' and 'cost' are interchangeable terms - you'll see them both around but I try to stick to 'cost'!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cost_space.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. The cost function is often referred to with different letters e.g. $J(w)$, $C(\\theta)$, $\\mathcal{L}(x)$, and $E(w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Fitting the Model & Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources to Watch and Read pt. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Watching/reading time:** ~1 hour\n",
    "\n",
    "First, watch these two videos from 3 Blue 1 Brown:\n",
    "1. [Gradient descent, how neural networks learn | Deep learning, chapter 2](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "2. [What is backpropagation and what is it actually doing? | Deep learning, chapter 3](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    "\n",
    "This will take you just over half an hour (if you watch at 1x speed). They are really excellent and well worth the time investment.\n",
    "\n",
    "Again, if you prefer reading try Nielsen's section [Learning with Gradient Descent](http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Best Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, we've got a function, let's call it $C(\\theta)$ that puts a number on how well the neural network is doing. We provide the function with the parameters $\\theta$ and it spits out the cost$^*$. We could just randomly chose values for $\\theta$ and select the ones that result in the best cost...but that might take a long time. We'd also need to define a way to randomly select parameters as well. What if the best parameter setting is very unlikely to be selected?\n",
    "\n",
    "**Calculus to the rescue!** The cost $C(\\theta)$ is a function and, whilst we can't see the surface without evaluating it everywhere (expensive!), we can calculate the derivative with respect to the parameters $\\frac{\\partial C(\\theta)}{\\partial \\theta}$. The derivative **tells you how the function value changes if you change $\\theta$**. \n",
    "\n",
    "For example, imagine $\\theta$ is 1D and I tell you that $\\frac{\\partial C(\\theta)}{\\partial \\theta} = 10\\theta$. This means that if I increase $theta$ by 2, the cost function will go up by 20. Which way will you update $\\theta$? You want to *decrease* the cost, so you would want to *decrease* $\\theta$ by some amount.\n",
    "\n",
    "The only thing we need to do is choose a cost function $C(\\theta)$ that has a derivative function $\\frac{\\partial C(\\theta)}{\\partial \\theta}$...and that is easy!\n",
    "\n",
    "$^*$It's much easier if you imagine $\\theta$ as just one number to start with, but the maths is basically the same as $\\theta$ becomes a vector (or matrix) of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we actually update the parameters?! All update the parameters in the opposite direction to the gradient; you always try to take a step 'downhill'. Here's the formula:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\frac{\\partial C(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "where \"$\\leftarrow$\" means \"update from\", and $\\eta$ is the \"learning rate\" - a hyperparameter you can choose. If you increase $\\eta$ you make bigger updates to $\\theta$, and vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more complicated ways to update the parameters using the gradient of the cost function, but they all have this same starting point.\n",
    "\n",
    "Below is an example cost surface. A few things to note:\n",
    "\n",
    "* The axes should be labelled $\\theta_0$ (1, -1.5) and $\\theta_1$ (-1, 1) on the 'flat' axes, and $C(\\theta)$ (-4, 4) on the vertical axis\n",
    "* The surface is shown - we don't have direct access to this in reality. To show it, the creator has queried the cost function *at every [$\\theta_0$, $\\theta_1$] location* and plotted it\n",
    "* The animated balls rolling along the surface are different gradient descent algorithms - each frame of the GIF shows one update. The equation shown above is SGD - the GIF highlights a potential issue with the algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://i.imgur.com/2dKCQHh.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Visualisation by [Alec Radford](https://blog.openai.com/tag/alec-radford/), summarised excellently in [this blog post](http://ruder.io/optimizing-gradient-descent/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading/watching time:** 1 hour\n",
    "\n",
    "Right...it's time for some derivatives. If you've been liking the videos - go ahead and watch the next in the series:\n",
    "\n",
    "1. [Backpropagation calculus | Appendix to deep learning chapter 3](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n",
    "\n",
    "If you have time, I recommend now having a crack at reading half of [Nielsen Chapter 2](http://neuralnetworksanddeeplearning.com/chap2.html), up to and including the section entitled [The Backpropagation Algorithm](http://neuralnetworksanddeeplearning.com/chap2.html#the_backpropagation_algorithm).\n",
    "\n",
    "I'm just going to write out some derivatives you're going to find useful for Part 3 below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "z^{(L)}                             &= W^{(L)}a^{(L-1)}  \\\\\n",
    "\\frac{\\partial z^{(L)}}{\\partial W} &= a^{(L-1)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{linear}[z]                             &= z \\\\\n",
    "\\frac{\\partial \\text{linear}[z]}{\\partial z} &= 1 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{sigmoid}[z] = \\sigma[z]                &= \\frac{1}{1 + e^{-z}} = \\frac{e^{z}}{e^{z} + 1}\\\\\n",
    "\\frac{\\partial \\sigma[z]}{\\partial z}        &= \\frac{e^{z}}{e^{z} + 1} - (\\frac{e^{z}}{e^{z} + 1})^2 \\\\\n",
    "                                             &= \\frac{e^{z}}{e^{z} + 1} ( 1 - \\frac{e^{z}}{e^{z} + 1} ) \\\\\n",
    "                                             &= \\sigma[z] (1 - \\sigma[z])\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{crossentropy}[y, a] = C[y, a]       &= - \\frac{1}{N} \\sum_{i=1}^N y_i \\log a_i + (1-y_i)\\log(1-a_i) \\\\\n",
    "\\frac{\\partial C[y_i, a_i]}{\\partial a_i} &=  \\frac{1 - y_i}{1 - a_i} + \\frac{y_i}{a_i}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And finally, this is all backpropagation really is...\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C[y_i, a_i]}{\\partial w_j} &=  \\frac{\\partial a_i}{\\partial w_j}\\frac{\\partial C[y_i, a_i]}{\\partial a_i}\\\\\n",
    "                                          &=  \\frac{\\partial z_k}{\\partial w_j}\\frac{\\partial a_i}{\\partial z_k}\\frac{\\partial C[y_i, a_i]}{\\partial a_i}\\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge: derive these yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on gradient based optimisers [check out this blog post](http://ruder.io/optimizing-gradient-descent/)\n",
    "\n",
    "For another look at backpropagation - try [Christopher Olah's blog](http://colah.github.io/posts/2015-08-Backprop/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Implementation From Scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 3.1 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing is first: **don't get stuck on this**. I recommend you attempt this question for an hour and, if you don't get anywhere, move on to Question 3.2. You can even move straight on to Part 4. It's exactly the same problem addressed here in 3.1, but using sklearn instead of coding it yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/network_design.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fit a very small neural network to classify the TC data. Here is the specification of the model:\n",
    "\n",
    "1. Input of size 9\n",
    "1. Hidden layer of size 3\n",
    "    * Linear activation function\n",
    "1. Output layer of size 1\n",
    "    * Logistic activation function\n",
    "\n",
    "As for the **cost function**: use Cross-Entropy. However, if you're getting bogged down with derivatives, feel free to try squared error to start with (this is what Nielsen and 3 Blue 1 Brown start with in their tutorials). Squared error is [not necessarily the right cost function to use](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/) but it will still work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given input vector $x$, we can predict an output probability $a^{(2)}$ (were the $^{(2)}$ indicates the layer number, *not a power* - I'm following 3 Blue 1 Brown notation as best I can) using the following formula:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{(2)} &= f^{(2)}[z^{(2)}] \\\\\n",
    "        &= f^{(2)}[W^{(2)}a^{(1)}] \\\\\n",
    "        &= f^{(2)}[W^{(2)}f^{(1)}[z^{(1)}]] \\\\\n",
    "        &= f^{(2)}[W^{(2)}f^{(1)}[W^{(1)}a^{(0)}]] \\\\\n",
    "        &= f^{(2)}[W^{(2)}f^{(1)}[W^{(1)}x]] \\\\\n",
    "        &= \\sigma[W^{(2)}(W^{(1)}x)]\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "where:\n",
    "\n",
    "* $f^{(2)}$ is the activation function of the output layer (a sigmoid function $\\sigma[]$)\n",
    "* $f^{(1)}$ is the activation function of the hidden layer (the identity - 'linear activation')\n",
    "* $W^{(2)}$ and $W^{(1)}$ are the parameters to learn\n",
    "* $a^{(L)} = f^{(L)}[z^{(L)}]$ are the activations **exiting** layer $^{(L)}$\n",
    "* $z^{(L)} = W^{(L)}a^{(L-1)}$ is the pre-activation weighted sum calculated **within** layer $^{(L)}$\n",
    "\n",
    "The formula for the Cross-Entropy cost function is:\n",
    "\n",
    "$$\n",
    "C(a) = - \\frac{1}{N} \\sum_{i=1}^N y_i \\log a_i + (1-y_i)\\log(1-a_i)\n",
    "$$\n",
    "\n",
    "Notice how only one term in the sum is ever non-zero because $y_i$ is only ever 0 or 1. In our case, $N$ is the number of data observations in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the model are two matrices:\n",
    "\n",
    "1. $W^{(2)}$ - $3 \\times 9$ matrix\n",
    "    * used within the hidden layer (the $1^{st}$ layer) to get $z^{(1)} = W^{(1)}x$ for some $9 \\times 1$ input vector $x$. $z^{(1)}$ is thus $3 \\times 1$.\n",
    "1. $W^{(1)}$ - $1 \\times 3$ matrix\n",
    "    * used within the output layer (the $2^{nd}$ layer) to get $z^{(2)} = W^{(2)}a^{(1)}$ for some $3 \\times 1$ input vector $a^{(1)}$. $z^{(2)}$ is thus $1 \\times 1$.\n",
    "\n",
    "**Note that I'm not asking you to fit *bias parameters*.**\n",
    "\n",
    "You'll often see parameters referred to as $\\theta$, it's a catch all term. In our case it's just a list of all the weights, $\\theta = [W^{(1)}, W^{(2)}]$. **We have 3 x 9 + 3 x 1 = 30 parameters to learn in total.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use any of the equations and code I've given you or linked you to in this lab but **you do not have to!** You're free to code as you please. Personally, since this is a simple example, I did not do anything fancy (I didn't create any objects with methods and attributes). I simply:\n",
    "* created a list containing the two parameter matrices `theta = [W1, W2]`\n",
    "* created a function to do prediction (the forward pass)\n",
    "* created a function to do the backward pass (updating the weights)\n",
    "    * This is the tricky bit - I coded functions that are the [relevant derivatives](#http://localhost:8888/notebooks/10_Lab_5_Neural_Networks.ipynb#Backpropagation), and wrote code to iteratively pass back the 'deltas' - (I think Nielsen's equations [here](http://neuralnetworksanddeeplearning.com/chap2.html#the_backpropagation_algorithm) are very useful)  \n",
    "* wrote a training loop which called these two main functions\n",
    "    * each epoch calls the forward pass to predict, then the backward pass to update the parameters.\n",
    "\n",
    "When the training was finished, my \"model\" was simply the parameters I had fitted, along with the 'forward pass' function - a function which uses those weights to predict a probability for any input data.\n",
    "\n",
    "**You do not have to code it up like me**, you can do it however you like! The point of this part is for you to explore, code up all the equations, understand how to calculate the loss, and how to use that loss to update the parameters of the model by backpropagation.\n",
    "\n",
    "**Debugging**: You're probably going to have issues particularly in the backprop section. You are welcome to make use of the `scipy.optimize.check_grad()` function. This takes as input a function f, g: a function that is (supposed to be) the function's derivative. \n",
    "\n",
    "If you didn't watch it already, now is a great time to take 10 minutes and watch [Backpropagation calculus | Appendix to deep learning chapter 3](https://www.youtube.com/watch?v=tIeHLnjs5U8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ===== What you actually need to do for this question! ====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a training loop which uses gradient descent to learn the parameters. Each iteration of the loop is called an **epoch**. Run your code for *no more than 100 epochs*. You should be able to achieve 100% accuracy on this problem. \n",
    "\n",
    "In this case, for simplicity, you may initialise the weights to be samples from a normal distribution mean 0 variance 1, but please note that this [is not necessarily good practice](https://intoli.com/blog/neural-network-initialization/)!\n",
    "\n",
    "**Do not code up a grid search for the learning rate hyperparameter**. You may instead play with the learning rate manually until you are happy. Try small values first like 0.0001 (if your backprop code is correct you **should** see your cost decreasing every epoch). Since this problem is so simple, a range of values should work. Again, with real data, you *must* do a search over hyperparameters, but here we are focussed on *coding* a working model.\n",
    "\n",
    "To test whether or not what you have written has worked, please output the following:\n",
    "1. After the training loop:\n",
    "    1. plot a graph of training and validation loss against epoch number\n",
    "    1. print or plot the final parameters you have learned using a Hinton diagram - feel free to use [code you can find online](http://bfy.tw/F74s)\n",
    "    1. pick one weight parameter and produce a plot of its value against epoch number\n",
    "        * Extension: do that for all the weights **leaving one specific input node** (i.e. the weights for one pixel of the input data)\n",
    "    1. use your model to:\n",
    "        1. print a few of the validation data examples and their predicted probabilities\n",
    "        1. print the output for a T and C with no noise (you can make that input data yourself)\n",
    "        1. print the output of a few random binary vectors i.e. 9x1 vectors of only 0s and 1s (again, you can make that input data yourself)\n",
    "\n",
    "1. Within the training loop:\n",
    "    1. print the training and validation crossentropy loss **and** percentage accuracy every epoch\n",
    "    1. save the value of the training and validation losses for every epoch [for the plot after the loop]\n",
    "    1. save the value of a weight parameter of your choice [for the plot after the loop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ===== Example outputs ====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I give you some examples of what I'd like you to produce. **I produced these using a learning rate of 0.003, 100 epochs, and weights initialised with N(0,1) with a random seed of 42**. I found that you could learn faster i.e. you can use a larger learning rate, but I wanted to make smooth plots for you. \n",
    "\n",
    "You don't need to produce plots exactly like this, you can do them how you like, but try and display the same information. You can also use my plots for checking (if you use the same settings as me)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cost_per_epoch.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/hinton_W1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/hinton_W2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/W1_x4__per_epoch.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/predict_valid_0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/predict_valid_No noise T.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/predict_valid_No noise C.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/predict_valid_N(0, 1) sample 1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/predict_valid_N(0, 1) sample 2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/training_log.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def identity(z):\n",
    "    return z\n",
    "\n",
    "def predict(X, params, activation_functions, return_all=False):\n",
    "    \n",
    "    \n",
    "    if return_all:\n",
    "        return [activations, zs]\n",
    "    else:\n",
    "        return activations[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backward pass (calculating the gradients and updating the parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 3.2 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you need a network this large to do this classification task? Give the values for the parameters of a network with no hidden layers, one output node, and an output activation function of a sigmoid that would get 100% accuracy. This network only has 9 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 3.3 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should recognise the model described in question 3.2. What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 3.4 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did I create input data, `X`, that was between [-1, 1] i.e. why wasn't it between [0, 1] like normal?! Would the model specified in question 3.1 above have worked if `X` was in [0, 1]? Explain why or why not.\n",
    "\n",
    "*Hint: if you're stuck, you can try it out by generating some new data and trying to fit it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 3.5 [EXTENSION] =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset which makes the problem harder. Have a look at the dataset generation code. You can use the arguments to create data with:\n",
    "* more letters (make the problem a multiclass classification)\n",
    "    * You'll need to implement the multiclass version of the sigmoid for the output activation function - [the softmax](https://en.wikipedia.org/wiki/Softmax_function) (and of course it's derivative) \n",
    "* increase the noise on the data\n",
    "\n",
    "Some other things you could implement:\n",
    "* include rotated letters in the data\n",
    "* make larger data (bigger than 3x3)\n",
    "* make the letters non-centred e.g. 5x5 data with 3x3 letters in 1 of 9 different places\n",
    "\n",
    "You'll probably need to adapt the code you wrote in 3.1, but you can probably copy and paste most of it. For an additional challenge: introduce [bias parameters](http://neuralnetworksanddeeplearning.com/chap1.html) and create your `X` data in range [0, 1] (i.e. set the bounds argument to [0, 1])...\n",
    "\n",
    "Some other things to try if you get code happy:\n",
    "* Implement stochastic gradient descent updates (updating parameters every training example, as opposed to every epoch) - tip: randomise data order each epoch\n",
    "* Implement batch gradient descent updates - tip: randomise data order each epoch\n",
    "\n",
    "**Requirements**:\n",
    "1. Describe the modelling problem and your input data. Plot some examples of the data\n",
    "1. Write down the model specification (I should be able to reproduce your model with this description):\n",
    "    * number of nodes in each layer\n",
    "    * a description of the parameters to learn (and a total number of parameters)\n",
    "    * the activation functions used for each layer\n",
    "    * cost function used\n",
    "1. All the outputs asked for in Question 3.1: loss per epoch plot, final parameters, a weight against epoch plot, and example predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Implementation with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 4.1 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did Question 3.1, this should be a breeze! Use the same data and perform the same modelling task. This time you can use Sklearn's Neural Network object [MLPClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier).\n",
    "\n",
    "\n",
    "Before you begin, read the [introduction](http://scikit-learn.org/stable/modules/neural_networks_supervised.html) (sections 1.17.1 and 1.17.2 at a minimum, 1.17.5, 1.17.6, 1.17.7 are recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 4.2 =========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned parameters are stored in the fitted sklearn [MLPClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) object **as two separate attributes**.\n",
    "\n",
    "1. Print the parameters learned by your fitted model\n",
    "1. Print the total number of parameters learned\n",
    "\n",
    "Look at the number of parameters described in question 3.1 (you do not need to have done this question 3.1 - just read its description). Below the code:\n",
    "\n",
    "1. Explain why the number of parameters learned by sklearn is different from the number specified in 3.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Please sir...I want some more](https://www.youtube.com/watch?v=Ex2r86G0sdc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, you successfully covered the basics of Neural Networks!\n",
    "\n",
    "If you enjoyed this lab, you'll love another course @ Edinburgh: [Machine Learning Practical](https://github.com/CSTR-Edinburgh/mlpractical). Check it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do, if you haven't already, is do the extension question 3.5. **In particular, you should implement bias parameters in your model code**.\n",
    "\n",
    "Next, go back to the very top of the notebook where I detail things I will not cover. Pick some words you don't understand (perhaps along with the keyword 'example' or 'introduction') and have fun reading/watching some tutorials about them online. Code up what you have learned; if you can code it up without peeking, you know you have understood it very well indeed. Another good \"starter for 10\" google is \"a review of neural networks for [images|text|music|bat detection|captioning images|generation|...]\".\n",
    "\n",
    "Here are some things that you might find fun to read:\n",
    "* [Visualising networks learning](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=5&networkShape=3&seed=0.42978&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "* [Trying to understand what features are learned by Deep Nets](https://distill.pub/2017/feature-visualization/)\n",
    "* [Modelling sound waves](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)\n",
    "   * ...and using that to [encode instruments](https://magenta.tensorflow.org/nsynth)\n",
    "* An [Introduction to LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) and their [unreasonable effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* How to encode the entire meaning of a word [in a few numbers](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n",
    "* [Convolutions for text data?!](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also:\n",
    "* [there](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)\n",
    "* [are](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "* [literally](https://www.coursera.org/learn/machine-learning)\n",
    "* [so](https://www.coursera.org/learn/neural-networks)\n",
    "* [many](http://deeplearning.net/)\n",
    "* [learning](http://datasciencemasters.org/)\n",
    "* [resources](https://metacademy.org/graphs/concepts/backpropagation)\n",
    "* [online!](http://www.deeplearningbook.org/)\n",
    "\n",
    "(about neural nets etc.)\n",
    "\n",
    "In all seriousness, make sure you check out [metacademy](https://metacademy.org/). You can search for a topic and it gives you a list of free resources, an estimated time you need to understand it, and prerequisite topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of this lab were inspired by  D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Parallel distributed processing: Explorations\n",
    "in the microstructure of cognition, vol. 1, MIT Press, Cambridge, MA, USA, 1986,\n",
    "pp. 318–362.\n",
    "\n",
    "\n",
    "Thanks also to:\n",
    "* [3 Blue 1 Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)\n",
    "* [Michael Nielsen](http://neuralnetworksanddeeplearning.com)\n",
    "* [Christopher Olah](http://colah.github.io/)\n",
    "\n",
    "for producing some excellent visualisations and learning resources and providing them free of charge.\n",
    "\n",
    "Additionally, many thanks to the developers of open source software, in particular:\n",
    "* [Numpy](http://www.numpy.org/)\n",
    "* [Scipy](https://www.scipy.org/)\n",
    "* [Sklearn](http://scikit-learn.org/stable/)\n",
    "* [Matplotlib](https://matplotlib.org/)\n",
    "* [Jupyter](http://jupyter.org/)\n",
    "* and of course [Python](https://www.python.org/) itself!\n",
    "\n",
    "your work is invaluable and appreciated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab was created by [James Owers](https://jamesowers.github.io/) in November 2017 and reviewed by [Patric Fulop](https://www.inf.ed.ac.uk/people/students/Patric_Fulop.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
